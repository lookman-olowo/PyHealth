{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GRASP: Mortality Prediction on MIMIC-III (Baseline — No code_mapping)\n\nThis notebook runs the GRASP model for mortality prediction **without** `code_mapping`.\nRaw ICD-9 and NDC codes are used as-is for the embedding vocabulary.\n\n**Paper**: Liantao Ma et al. \"GRASP: Generic Framework for Health Status Representation Learning Based on Incorporating Knowledge from Similar Patients.\" AAAI 2021.\n\nGRASP encodes patient sequences with a backbone (ConCare, GRU, or LSTM), clusters patients via k-means, refines cluster representations with a 2-layer GCN, and blends cluster-level knowledge back into individual patient representations via a learned gating mechanism.\n\n**Model:** GRASP (GRU backbone + GCN cluster refinement)  \n**Task:** In-hospital mortality prediction  \n**Dataset:** Synthetic MIMIC-III (`dev=False`)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Load the MIMIC-III Dataset\n\nWe load the MIMIC-III dataset using PyHealth's `MIMIC3Dataset` class. We use the synthetic dataset hosted on GCS, which requires no credentials.\n\n- `root`: URL to the synthetic MIMIC-III data\n- `tables`: Clinical tables to load (diagnoses, procedures, prescriptions)\n- `dev`: Set to `False` for the full dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "base_dataset = MIMIC3Dataset(\n",
    "    root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III\",\n",
    "    tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"],\n",
    "    cache_dir=tempfile.TemporaryDirectory().name,\n",
    "    dev=False,\n",
    ")\n",
    "\n",
    "base_dataset.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Define the Mortality Prediction Task\n\nThe `MortalityPredictionMIMIC3` task extracts samples from the raw EHR data:\n- Extracts diagnosis codes (ICD-9), procedure codes, and drug information from each visit\n- Creates binary labels based on in-hospital mortality\n- Filters out visits without sufficient clinical codes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.tasks import MortalityPredictionMIMIC3\n",
    "\n",
    "task = MortalityPredictionMIMIC3()\n",
    "\n",
    "samples = base_dataset.set_task(task)\n",
    "\n",
    "print(f\"Generated {len(samples)} samples\")\n",
    "print(f\"\\nInput schema: {samples.input_schema}\")\n",
    "print(f\"Output schema: {samples.output_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Dataset Statistics\n\nEach sample represents one hospital visit with:\n- **conditions**: List of ICD-9 diagnosis codes\n- **procedures**: List of ICD-9 procedure codes\n- **drugs**: List of NDC drug codes\n- **mortality**: Binary label (0 = survived, 1 = deceased)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample structure:\")\n",
    "print(samples[0])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Processor Vocabulary Sizes:\")\n",
    "print(\"=\" * 50)\n",
    "for key, proc in samples.input_processors.items():\n",
    "    if hasattr(proc, 'code_vocab'):\n",
    "        print(f\"{key}: {len(proc.code_vocab)} codes (including <pad>, <unk>)\")\n",
    "\n",
    "mortality_count = sum(float(s.get(\"mortality\", 0)) for s in samples)\n",
    "print(f\"\\nTotal samples: {len(samples)}\")\n",
    "print(f\"Mortality rate: {mortality_count / len(samples) * 100:.2f}%\")\n",
    "print(f\"Positive samples: {int(mortality_count)}\")\n",
    "print(f\"Negative samples: {len(samples) - int(mortality_count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Split the Dataset\n\nWe split the data by patient to avoid data leakage — all visits from a given patient go into the same split."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import split_by_patient\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(\n",
    "    samples, [0.8, 0.1, 0.1], seed=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Create Data Loaders\n\nData loaders batch the samples and handle data feeding during training and evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import get_dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(val_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Initialize the GRASP Model\n\nThe GRASP model automatically handles different feature types via `EmbeddingModel`.\nSequence features (diagnosis/procedure/drug codes) are embedded using learned embeddings,\nand each feature gets its own `GRASPLayer`.\n\n### Key Parameters:\n- `embedding_dim`: Dimension of code embeddings (default: 128)\n- `hidden_dim`: Hidden dimension of the backbone (default: 128)\n- `cluster_num`: Number of patient clusters for knowledge sharing (default: 2)\n- `block`: Backbone encoder — `\"ConCare\"`, `\"GRU\"`, or `\"LSTM\"` (default: `\"ConCare\"`)\n- `dropout`: Dropout rate for regularization (default: 0.5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import GRASP\n",
    "\n",
    "model = GRASP(\n",
    "    dataset=samples,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    cluster_num=12,\n",
    "    block=\"GRU\",\n",
    ")\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Train the Model\n\nWe use PyHealth's `Trainer` class which handles:\n- Training loop with automatic batching\n- Validation during training\n- Model checkpointing based on validation metrics\n\nWe monitor the **ROC-AUC** score on the validation set."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    metrics=[\"roc_auc\", \"pr_auc\", \"accuracy\", \"f1\"],\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=50,\n",
    "    monitor=\"roc_auc\",\n",
    "    optimizer_params={\"lr\": 1e-3},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Evaluate on Test Set\n\nAfter training, we evaluate the model on the held-out test set to measure its generalization performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.evaluate(test_dataloader)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test Set Performance (NO code_mapping)\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in test_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 9: Extract Patient Embeddings\n\nGRASP produces patient embeddings that encode health status enriched with knowledge from similar patients.\nThese embeddings can be used for downstream tasks like patient similarity search, cohort discovery, or transfer learning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\n\nmodel.eval()\ntest_batch = next(iter(test_dataloader))\ntest_batch[\"embed\"] = True\n\nwith torch.no_grad():\n    output = model(**test_batch)\n\nprint(f\"Embedding shape: {output['embed'].shape}\")\nprint(f\"  - Batch size: {output['embed'].shape[0]}\")\nprint(f\"  - Embedding dim: {output['embed'].shape[1]}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Sample Predictions:\")\nprint(\"=\" * 50)\npredictions = output[\"y_prob\"].cpu().numpy()\ntrue_labels = output[\"y_true\"].cpu().numpy()\n\nfor i in range(min(5, len(predictions))):\n    pred = predictions[i][0]\n    true = int(true_labels[i][0])\n    print(f\"Patient {i + 1}: Predicted={pred:.3f}, True={true}, Prediction={'Mortality' if pred > 0.5 else 'Survival'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}